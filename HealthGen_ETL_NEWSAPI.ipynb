{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e90224bf-fc0b-40df-b53a-66d888597a75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql import SparkSession\n",
    "import requests as req\n",
    "import datetime as dt\n",
    "import pyspark.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0ec6ad-6828-42e2-a4a9-fadbe7ded42a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_data(url, acess_code, searched_word, initial_data = None, final_data = None):\n",
    "    \"\"\"\n",
    "    Realiza extração dos dados da API da NEWSAPI com base dos parâmetros recebidos\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        String do método get da API.\n",
    "\n",
    "    acess_code: string\n",
    "        String com a chave usada para coletar os dados da API.\n",
    "\n",
    "    searched_word: string\n",
    "        String com a palavra a ser buscada na notícia.\n",
    "        \n",
    "    initial_data: date\n",
    "        Data inicial a ser buscada, no formato 'YYYY-MM-DD'.\n",
    "\n",
    "    final_data: date\n",
    "        Data final a ser buscada, 'YYYY-MM-DD'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response_df: float\n",
    "        DataFrame com o retorno dos dados da API\n",
    "\n",
    "    \"\"\"\n",
    "    if initial_data == None:\n",
    "        initial_data = dt.datetime.now().date() - dt.timedelta(days=1)\n",
    "\n",
    "    if final_data == None:\n",
    "        final_data = dt.datetime.now().date() - dt.timedelta(days=1)\n",
    "\n",
    "    parameters = {\n",
    "        'q': searched_word,\n",
    "        'apiKey': acess_code,\n",
    "        'sortBy': 'publishedAt',\n",
    "        'from': initial_data,\n",
    "        'to': final_data\n",
    "    }\n",
    "\n",
    "    response = req.get(url, params = parameters)\n",
    "    response_dic = response.json()\n",
    "\n",
    "    if response_dic['status'] != 'ok':\n",
    "        messageerror = response_dic['message']\n",
    "        raise Exception(messageerror)\n",
    "        \n",
    "    dict_res = {}\n",
    "    #percorre todas as noticias encontradas no resultado da chamada para criar uma chave única\n",
    "    for item in response_dic['articles']:\n",
    "        chave = item['url'] + item['publishedAt']\n",
    "        dict_res[chave] = {\n",
    "            \"source\": item['source']\n",
    "            ,\"author\": item['author']\n",
    "            ,\"title\": item['title']\n",
    "            ,\"description\": item['description']\n",
    "            ,\"url\":  item['url']\n",
    "            ,\"urlToImage\": item['urlToImage']\n",
    "            ,\"publishedAt\": item['publishedAt']\n",
    "            ,\"content\": item['content']\n",
    "        }\n",
    "\n",
    "    response_df = pd.DataFrame.from_dict(dict_res, orient='index').reset_index().rename(columns={\"index\": \"id\"})\n",
    "    return response_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5ebd75-1834-4fe8-8945-e2c3c88d0887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data(new_data, path, file_name, key):\n",
    "    \"\"\"\n",
    "    Realiza a leitura do arquivo com os dados já salvo e adiciona os novos dados\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    new_data: DataFrame\n",
    "        Dataframe contendo os dados coletados da API.\n",
    "\n",
    "    path: string\n",
    "        Diretório dentro do dbfs onde os dados serão salvos.\n",
    "\n",
    "    file_name: string\n",
    "        Nome do arquivo em formato parquet onde os dados serão salvos\n",
    "\n",
    "    key: string\n",
    "        Nome do campo que identifica o registro como único\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    #caso já exista o arquivo transformado, segue direto com a concatenação e com a carga do resultado final\n",
    "    try: \n",
    "        complete_path = path + file_name\n",
    "        arquivo = dbutils.fs.ls(complete_path)\n",
    "        df_res = pd.read_parquet(complete_path)\n",
    "        df_res = pd.concat([df_res, new_data])\n",
    "        df_res.to_parquet(complete_path)\n",
    "\n",
    "         #elimina os registros duplicados baseado na chave passada antes de armazenar o resultado final\n",
    "        df_res = df_res.sort_values(by=\"publishedAt\")\n",
    "        df_res = df_res.drop_duplicates(subset=key, keep='last')\n",
    "\n",
    "        #armazena o resultado final\n",
    "        df_res.to_parquet(complete_path)\n",
    "    #caso o arquivo transformando ainda não exista, quer dizer que é o primeiro processo do pipeline e é preciso criar o arquivo destino\n",
    "    except Exception as e: \n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(\"Arquivo não encontrado, primeiro processamento\")\n",
    "            new_data.to_parquet(complete_path)\n",
    "\n",
    "    print(\"resultado carregado com sucesso\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814b78da-3ec9-469d-ae6a-c07bcee6541d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_folder(path):\n",
    "    \"\"\"\n",
    "    Realiza verificação de existência do diretório onde os dados serão salvos\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        Diretório dentro do dbfs onde os dados serão salvos.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    try:#Verifica se o diretório passado existe. Caso não, cria\n",
    "        dbutils.fs.ls(path)\n",
    "    except Exception as e:\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(\"Diretório não existente. Criando diretório.\")\n",
    "            dbutils.fs.mkdirs(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072efcdb-c35f-4be3-879e-081420aba00a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#define o diretório de qual será o arquivo parquet final que irá armazenar os dados carregados\n",
    "path = \"/dbfs/data_newsapi/teste/\"\n",
    "file_name = \"data_newsapi.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae51c816-ab18-4a56-9587-85bd43ce4fac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:467: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n  warn(msg)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo não encontrado, primeiro processamento\nresultado carregado com sucesso\n"
     ]
    }
   ],
   "source": [
    "url = \"https://newsapi.org/v2/everything?\"\n",
    "acess_code = \"ed08e4049379498ebde784dee9d1ede8\"\n",
    "searched_word = \"cancer OR DNA OR genetic\"\n",
    "key = \"id\"\n",
    "\n",
    "response_df = extract_data(url, acess_code, searched_word)\n",
    "check_folder(path)\n",
    "load_data(response_df,path,file_name,key)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HealthGen_ETL_NEWSAPI",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
